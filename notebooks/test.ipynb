{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42812342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piyus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import thop  # For FLOPs profiling\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61452f7c",
   "metadata": {},
   "source": [
    "Dataset Setup (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b962d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "fashion_train = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "fashion_test = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "cifar_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "cifar_train = datasets.CIFAR10(root='./data', train=True, transform=cifar_transform, download=True)\n",
    "cifar_test = datasets.CIFAR10(root='./data', train=False, transform=cifar_transform, download=True)\n",
    "\n",
    "mnist_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)\n",
    "\n",
    "fashion_loader = DataLoader(fashion_train, batch_size=64, shuffle=True)\n",
    "fashion_test_loader = DataLoader(fashion_test, batch_size=64, shuffle=False)\n",
    "\n",
    "cifar_loader = DataLoader(cifar_train, batch_size=64, shuffle=True)\n",
    "cifar_test_loader = DataLoader(cifar_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78af3c7a",
   "metadata": {},
   "source": [
    "Define Norm Alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae962d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DyTNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones(dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        return self.alpha * (x - mean) + self.beta\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = x.norm(dim=-1, keepdim=True) / (x.size(-1) ** 0.5)\n",
    "        return self.weight * (x / (rms + self.eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a686e0",
   "metadata": {},
   "source": [
    "NormSelector Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30373312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormSelector(nn.Module):\n",
    "    def __init__(self, dim, freeze_dyt=False, freeze_ln=False):\n",
    "        super().__init__()\n",
    "        self.norms = nn.ModuleList([\n",
    "            DyTNorm(dim),\n",
    "            nn.LayerNorm(dim),\n",
    "            RMSNorm(dim)\n",
    "        ])\n",
    "        self.weights = nn.Parameter(torch.ones(3))\n",
    "        self.freeze_dyt = freeze_dyt\n",
    "        self.freeze_ln = freeze_ln\n",
    "        self.last_grad_norm = torch.zeros(3)\n",
    "        self.grad_history = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        ws = F.softmax(self.weights, dim=0)\n",
    "        norms_out = [norm(x) for norm in self.norms]\n",
    "\n",
    "        if self.freeze_dyt:\n",
    "            ws = ws.clone()\n",
    "            ws[0] = 0.0\n",
    "        if self.freeze_ln:\n",
    "            ws = ws.clone()\n",
    "            ws[1] = 0.0\n",
    "\n",
    "        ws = ws / ws.sum()\n",
    "        out = sum(w * norm for w, norm in zip(ws, norms_out))\n",
    "\n",
    "        if self.training and x.requires_grad:\n",
    "            with torch.no_grad():\n",
    "                for i, norm in enumerate(norms_out):\n",
    "                    g = norm.detach().norm()\n",
    "                    self.last_grad_norm[i] = g\n",
    "                self.grad_history.append(self.last_grad_norm.clone())\n",
    "\n",
    "        return out\n",
    "\n",
    "    def show_norm_weights(self):\n",
    "        ws = F.softmax(self.weights, dim=0)\n",
    "        return {'DyT': ws[0].item(), 'LN': ws[1].item(), 'RMS': ws[2].item()}\n",
    "\n",
    "    def show_gradient_norms(self):\n",
    "        return self.last_grad_norm.tolist()\n",
    "\n",
    "    def plot_gradient_norms(self, tag=''):\n",
    "        arr = torch.stack(self.grad_history).cpu().numpy()\n",
    "        plt.figure()\n",
    "        plt.plot(arr[:, 0], label='DyT GradNorm')\n",
    "        plt.plot(arr[:, 1], label='LN GradNorm')\n",
    "        plt.plot(arr[:, 2], label='RMS GradNorm')\n",
    "        plt.legend()\n",
    "        plt.title(f'Gradient Norms per Layer - {tag}')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Grad Norm')\n",
    "        plt.savefig(f'plot_gradnorms_{tag}.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3222b38",
   "metadata": {},
   "source": [
    "Transformer Block with AutoNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a7f779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads=2, ff_dim=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = NormSelector(dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, dim)\n",
    "        )\n",
    "        self.norm2 = NormSelector(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + x2)\n",
    "        x2 = self.ff(x)\n",
    "        x = self.norm2(x + x2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5c631",
   "metadata": {},
   "source": [
    "Teacher Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82f704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockTeacher(nn.Module):\n",
    "    def __init__(self, dim, heads=2, ff_dim=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + x2)\n",
    "        x2 = self.ff(x)\n",
    "        x = self.norm2(x + x2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc80b593",
   "metadata": {},
   "source": [
    "Transformer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f194a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=28, model_dim=64, num_classes=10, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.layers = nn.Sequential(*[\n",
    "            TransformerBlock(model_dim, dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(model_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.layers(x)\n",
    "        return self.classifier(x.mean(dim=1))\n",
    "\n",
    "class SimpleTransformerTeacher(nn.Module):\n",
    "    def __init__(self, input_dim=28, model_dim=64, num_classes=10, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.layers = nn.Sequential(*[\n",
    "            TransformerBlockTeacher(model_dim, dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(model_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.layers(x)\n",
    "        return self.classifier(x.mean(dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff27f44",
   "metadata": {},
   "source": [
    "Save + Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4859a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model_class, path, *args, **kwargs):\n",
    "    model = model_class(*args, **kwargs)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fcf8d6",
   "metadata": {},
   "source": [
    "Train & Evaluate Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b68e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(dataset_name, batch_size=64):\n",
    "    transform = transforms.ToTensor()\n",
    "    if dataset_name == 'MNIST':\n",
    "        dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "        testset = datasets.MNIST('data', train=False, transform=transform)\n",
    "    elif dataset_name == 'FashionMNIST':\n",
    "        dataset = datasets.FashionMNIST('data', train=True, download=True, transform=transform)\n",
    "        testset = datasets.FashionMNIST('data', train=False, transform=transform)\n",
    "    elif dataset_name == 'CIFAR10':\n",
    "        transform = transforms.Compose([transforms.Grayscale(), transforms.ToTensor()])\n",
    "        dataset = datasets.CIFAR10('data', train=True, download=True, transform=transform)\n",
    "        testset = datasets.CIFAR10('data', train=False, transform=transform)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset\")\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True), DataLoader(testset, batch_size=batch_size)\n",
    "\n",
    "def train_model(model, dataloader, optimizer, epochs=5, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for img, label in dataloader:\n",
    "            img, label = img.to(device), label.to(device)\n",
    "            img = img.squeeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        loss_list.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "    return loss_list\n",
    "\n",
    "def evaluate_model(model, dataloader, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for img, label in dataloader:\n",
    "            img, label = img.to(device), label.to(device)\n",
    "            img = img.squeeze(1)\n",
    "            out = model(img)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == label).sum().item()\n",
    "            total += label.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def run_experiment(model_class, save_path, dataset_train='MNIST', dataset_test='FashionMNIST'):\n",
    "    print(f\"Training on {dataset_train}, testing on {dataset_test}\")\n",
    "    train_loader, _ = get_loader(dataset_train)\n",
    "    _, test_loader = get_loader(dataset_test)\n",
    "\n",
    "    model = model_class()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    train_model(model, train_loader, optimizer, epochs=5)\n",
    "    save_model(model, save_path)\n",
    "    acc = evaluate_model(model, test_loader)\n",
    "    print(f\"Transfer Accuracy on {dataset_test}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dd25c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on MNIST, testing on FashionMNIST\n",
      "Epoch 1, Loss: 0.8084\n",
      "Epoch 2, Loss: 0.4795\n",
      "Epoch 3, Loss: 0.4080\n",
      "Epoch 4, Loss: 0.3646\n",
      "Epoch 5, Loss: 0.3340\n",
      "Transfer Accuracy on FashionMNIST: 0.0427\n"
     ]
    }
   ],
   "source": [
    "run_experiment(SimpleTransformer, 'autonorm_mnist.pth', 'MNIST', 'FashionMNIST')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa9f1d",
   "metadata": {},
   "source": [
    "FINE TUNING \\n\n",
    "RESULTS \\n\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9a319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
